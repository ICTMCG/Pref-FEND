dataset	Weibo
category_num	2
save	ckpts/DeClarE_with_Pref-FENDs
use_preference_map	True
use_pattern_based_model	False
use_fact_based_model	True
num_gnn_layers	2
dim_node_features	768
updated_weights_for_A	0.5
pattern_based_model	
fact_based_model	DeClarE
output_dim_of_pattern_based_model	0
output_dim_of_fact_based_model	256
num_mlp_layers	3
weight_of_normal_loss	2.0
weight_of_preference_loss	1.0
weight_of_reversed_loss	1.0
bilstm_input_max_sequence_length	100
bilstm_input_dim	768
bilstm_hidden_dim	128
bilstm_num_layer	1
bilstm_dropout	0
eann_input_max_sequence_length	100
eann_input_dim	768
eann_hidden_dim	64
eann_event_num	300
eann_use_textcnn	True
eann_weight_of_event_loss	-1.0
bert_pretrained_model_chinese	bert-base-chinese
bert_pretrained_model_english	bert-base-uncased
bert_input_max_sequence_length	100
bert_training_embedding_layers	True
bert_training_inter_layers	True
bert_emotion_dim	0
bert_hidden_dim	768
mac_input_max_sequence_length	100
mac_max_doc_length	200
mac_input_dim	768
mac_hidden_dim	300
mac_dropout_doc	0
mac_dropout_query	0
mac_nhead_1	5
mac_nhead_2	2
evin_input_max_sequence_length	200
evin_max_doc_length	200
evin_input_dim	768
evin_hidden_dim	60
evin_dropout_att	0.5
evin_dropout_mlp	0.6
evin_nhead	6
declare_input_max_sequence_length	100
declare_input_dim	768
declare_hidden_dim	128
declare_max_doc_length	200
declare_bilstm_num_layer	1
declare_bilstm_dropout	0
lr	0.001
epochs	50
batch_size	8
start_epoch	0
resume	
evaluate	False
debug	False
seed	9
device	cuda
fp16	True
local_rank	-1

PrefFEND(
  (FactBasedModel): DeClarE(
    (post_bilstm): LSTM(768, 128, batch_first=True, bidirectional=True)
    (articles_bilstm): LSTM(768, 128, batch_first=True, bidirectional=True)
    (Wa): Linear(in_features=512, out_features=1, bias=True)
    (Wc): Linear(in_features=256, out_features=256, bias=True)
    (fc): Linear(in_features=256, out_features=256, bias=True)
  )
  (fcs): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
  (HetDGCN): HetDGCN(
    (gnn_layers): ModuleList(
      (0): ModuleDict(
        (ENTITY): GCNConv(768, 768)
        (OTHERS): GCNConv(768, 768)
        (PATTERN): GCNConv(768, 768)
      )
      (1): ModuleDict(
        (ENTITY): GCNConv(768, 768)
        (OTHERS): GCNConv(768, 768)
        (PATTERN): GCNConv(768, 768)
      )
    )
    (gnn_dynamic_update_weights): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 768x768 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 768x768 (GPU 0)]
    )
  )
  (reversed_fcs): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
