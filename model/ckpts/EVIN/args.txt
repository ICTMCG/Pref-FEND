dataset	Weibo
category_num	2
save	ckpts/EVIN
use_preference_map	False
use_pattern_based_model	False
use_fact_based_model	True
num_gnn_layers	2
dim_node_features	768
updated_weights_for_A	0.5
pattern_based_model	
fact_based_model	EVIN
output_dim_of_pattern_based_model	0
output_dim_of_fact_based_model	256
num_mlp_layers	3
weight_of_normal_loss	1.0
weight_of_preference_loss	0.0
weight_of_reversed_loss	0.0
bilstm_input_max_sequence_length	100
bilstm_input_dim	768
bilstm_hidden_dim	128
bilstm_num_layer	1
bilstm_dropout	0
eann_input_max_sequence_length	100
eann_input_dim	768
eann_hidden_dim	64
eann_event_num	300
eann_use_textcnn	True
eann_weight_of_event_loss	1.0
bert_pretrained_model	None
bert_input_max_sequence_length	100
bert_training_embedding_layers	True
bert_training_inter_layers	True
bert_emotion_dim	0
bert_hidden_dim	768
declare_input_max_sequence_length	100
declare_input_dim	768
declare_hidden_dim	128
declare_max_doc_length	200
declare_bilstm_num_layer	1
declare_bilstm_dropout	0
evin_input_max_sequence_length	100
evin_max_doc_length	200
evin_input_dim	768
evin_hidden_dim	60
evin_dropout_att	0.5
evin_dropout_mlp	0.6
evin_nhead	6
mac_input_max_sequence_length	100
mac_max_doc_length	200
mac_input_dim	768
mac_hidden_dim	300
mac_dropout_doc	0
mac_dropout_query	0
mac_nhead_1	5
mac_nhead_2	2
lr	0.0001
epochs	50
batch_size	8
start_epoch	0
resume	
evaluate	False
debug	False
seed	9
device	cuda
fp16	True
local_rank	-1

PrefFEND(
  (FactBasedModel): EVIN(
    (G1): GateAffineAbsorpModule(
      (alpha): Sequential(
        (0): Linear(in_features=120, out_features=120, bias=True)
        (1): Tanh()
        (2): Linear(in_features=120, out_features=120, bias=True)
      )
      (beta): Sequential(
        (0): Linear(in_features=120, out_features=120, bias=True)
        (1): Tanh()
      )
      (beta2): Linear(in_features=120, out_features=120, bias=True)
      (gamma): Linear(in_features=120, out_features=120, bias=True)
    )
    (G2): GateAffineAbsorpModule(
      (alpha): Sequential(
        (0): Linear(in_features=120, out_features=120, bias=True)
        (1): Tanh()
        (2): Linear(in_features=120, out_features=120, bias=True)
      )
      (beta): Sequential(
        (0): Linear(in_features=120, out_features=120, bias=True)
        (1): Tanh()
      )
      (beta2): Linear(in_features=120, out_features=120, bias=True)
      (gamma): Linear(in_features=120, out_features=120, bias=True)
    )
    (mhatt): MultiHeadAttention(
      (query): Linear(in_features=120, out_features=120, bias=True)
      (key): Linear(in_features=120, out_features=120, bias=True)
      (value): Linear(in_features=120, out_features=120, bias=True)
      (softmax): Softmax(dim=-1)
      (dropout): Dropout(p=0.5, inplace=False)
      (dense): Linear(in_features=120, out_features=120, bias=True)
    )
    (doc_all_bilstm): LSTM(768, 60, batch_first=True, bidirectional=True)
    (doc_single_bilstm): LSTM(768, 60, batch_first=True, bidirectional=True)
    (query_bilstm): LSTM(768, 60, batch_first=True, bidirectional=True)
    (ecl_query_bilstm): LSTM(120, 60, batch_first=True, bidirectional=True)
    (ecl_doc_bilstm): LSTM(120, 60, batch_first=True, bidirectional=True)
    (mlp1): Linear(in_features=240, out_features=240, bias=False)
    (fc): Linear(in_features=240, out_features=256, bias=True)
  )
  (fcs): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
