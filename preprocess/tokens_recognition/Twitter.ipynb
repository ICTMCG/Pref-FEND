{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:07.768825Z",
     "start_time": "2021-11-23T06:47:07.763005Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:08.329620Z",
     "start_time": "2021-11-23T06:47:07.770457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8822\n",
      "val 2943\n",
      "test 2944\n"
     ]
    }
   ],
   "source": [
    "# Here, the lists do not contain segmented and tagged words.\n",
    "datasets = []\n",
    "for t in ['train', 'val', 'test']:\n",
    "    with open('../../dataset/Twitter/raw/post/{}.json'.format(t), 'r') as f:\n",
    "        pieces = json.load(f)\n",
    "        print(t, len(pieces))\n",
    "        datasets.append(pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:09.573700Z",
     "start_time": "2021-11-23T06:47:09.557788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21126, ['!', '！', '?', '？', ',', '，', '.', '。', '[', '【'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_words = []\n",
    "with open('./resources/pattern_words_English.txt', 'r') as f:\n",
    "    pattern_words = f.readlines()\n",
    "    pattern_words = [l.strip() for l in pattern_words]\n",
    "len(pattern_words), pattern_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used the API from TexSmart, but it seems unavailable now.\n",
    "# You may use its SDK version.\n",
    "# See https://ai.tencent.com/ailab/nlp/texsmart/zh/index.html#instructions\n",
    "api = \"https://texsmart.qq.com/api\"\n",
    "\n",
    "opt = {\n",
    "    \"input_spec\":{\"lang\":\"en\"},\n",
    "    \"word_seg\":{\"enable\":True},\n",
    "    \"pos_tagging\":{\"enable\":True,\"alg\":\"crf\"},\n",
    "    \"ner\":{\"enable\":True,\"alg\":\"crf\",\"fine_grained\":False},\n",
    "    \"syntactic_parsing\":{\"enable\":False},\n",
    "    \"srl\":{\"enable\":False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:09.580630Z",
     "start_time": "2021-11-23T06:47:09.575114Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_a_text(index, text):\n",
    "    req_str = json.dumps(\n",
    "        {\n",
    "            'str':text,\n",
    "            'options':opt,\n",
    "            'echo_data':index\n",
    "        }\n",
    "    ).encode()\n",
    "\n",
    "    r = requests.post(api, data=req_str)\n",
    "    r.encoding = \"utf-8\"\n",
    "    res = json.loads(r.text)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:41.345402Z",
     "start_time": "2021-11-23T06:47:41.319999Z"
    }
   },
   "outputs": [],
   "source": [
    "t = random.sample(random.sample(datasets, 1)[0], 1)[0]\n",
    "print(t['label'])\n",
    "print()\n",
    "\n",
    "handle_a_text(0, t['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example\n",
    "# {'header': {'time_cost_ms': 56.465,\n",
    "#   'time_cost': 0.056465,\n",
    "#   'core_time_cost_ms': 56.383,\n",
    "#   'ret_code': 'succ'},\n",
    "#   'norm_str': 'It took a few seasons, but the first couple to sleep in the same bed on TV was Fred and Wilma. Yabba Dabba Do! ',\n",
    "#   'lang': 'en',\n",
    "#   'word_list': [{'str': 'It', 'hit': [0, 2, 0, 1], 'tag': 'PRP'},\n",
    "#   {'str': 'took', 'hit': [3, 4, 1, 1], 'tag': 'VBD'},\n",
    "#   {'str': 'a', 'hit': [8, 1, 2, 1], 'tag': 'DT'},\n",
    "#   {'str': 'few', 'hit': [10, 3, 3, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'seasons', 'hit': [14, 7, 4, 1], 'tag': 'NNS'},\n",
    "#   {'str': ',', 'hit': [21, 1, 5, 1], 'tag': ','},\n",
    "#   {'str': 'but', 'hit': [23, 3, 6, 1], 'tag': 'CC'},\n",
    "#   {'str': 'the', 'hit': [27, 3, 7, 1], 'tag': 'DT'},\n",
    "#   {'str': 'first', 'hit': [31, 5, 8, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'couple', 'hit': [37, 6, 9, 1], 'tag': 'NN'},\n",
    "#   {'str': 'to', 'hit': [44, 2, 10, 1], 'tag': 'IN'},\n",
    "#   {'str': 'sleep', 'hit': [47, 5, 11, 1], 'tag': 'NN'},\n",
    "#   {'str': 'in', 'hit': [53, 2, 12, 1], 'tag': 'IN'},\n",
    "#   {'str': 'the', 'hit': [56, 3, 13, 1], 'tag': 'DT'},\n",
    "#   {'str': 'same', 'hit': [60, 4, 14, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'bed', 'hit': [65, 3, 15, 1], 'tag': 'NN'},\n",
    "#   {'str': 'on', 'hit': [69, 2, 16, 1], 'tag': 'IN'},\n",
    "#   {'str': 'TV', 'hit': [72, 2, 17, 1], 'tag': 'NN'},\n",
    "#   {'str': 'was', 'hit': [75, 3, 18, 1], 'tag': 'VBD'},\n",
    "#   {'str': 'Fred', 'hit': [79, 4, 19, 1], 'tag': 'NNP'},\n",
    "#   {'str': 'and', 'hit': [84, 3, 20, 1], 'tag': 'CC'},\n",
    "#   {'str': 'Wilma', 'hit': [88, 5, 21, 1], 'tag': 'NNP'},\n",
    "#   {'str': '.', 'hit': [93, 1, 22, 1], 'tag': '.'},\n",
    "#   {'str': 'Yabba', 'hit': [95, 5, 23, 1], 'tag': 'NNP'},\n",
    "#   {'str': 'Dabba', 'hit': [101, 5, 24, 1], 'tag': 'NNP'},\n",
    "#   {'str': 'Do', 'hit': [107, 2, 25, 1], 'tag': 'NNP'},\n",
    "#   {'str': '!', 'hit': [109, 1, 26, 1], 'tag': '.'}],\n",
    "#   'phrase_list': [{'str': 'It', 'hit': [0, 2, 0, 1], 'tag': 'PRP'},\n",
    "#   {'str': 'took', 'hit': [3, 4, 1, 1], 'tag': 'VBD'},\n",
    "#   {'str': 'a', 'hit': [8, 1, 2, 1], 'tag': 'DT'},\n",
    "#   {'str': 'few', 'hit': [10, 3, 3, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'seasons', 'hit': [14, 7, 4, 1], 'tag': 'NNS'},\n",
    "#   {'str': ',', 'hit': [21, 1, 5, 1], 'tag': ','},\n",
    "#   {'str': 'but', 'hit': [23, 3, 6, 1], 'tag': 'CC'},\n",
    "#   {'str': 'the', 'hit': [27, 3, 7, 1], 'tag': 'DT'},\n",
    "#   {'str': 'first', 'hit': [31, 5, 8, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'couple', 'hit': [37, 6, 9, 1], 'tag': 'NN'},\n",
    "#   {'str': 'to', 'hit': [44, 2, 10, 1], 'tag': 'IN'},\n",
    "#   {'str': 'sleep', 'hit': [47, 5, 11, 1], 'tag': 'NN'},\n",
    "#   {'str': 'in', 'hit': [53, 2, 12, 1], 'tag': 'IN'},\n",
    "#   {'str': 'the', 'hit': [56, 3, 13, 1], 'tag': 'DT'},\n",
    "#   {'str': 'same', 'hit': [60, 4, 14, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'bed', 'hit': [65, 3, 15, 1], 'tag': 'NN'},\n",
    "#   {'str': 'on TV', 'hit': [69, 5, 16, 2], 'tag': 'IN'},\n",
    "#   {'str': 'was', 'hit': [75, 3, 18, 1], 'tag': 'VBD'},\n",
    "#   {'str': 'Fred', 'hit': [79, 4, 19, 1], 'tag': 'JJ'},\n",
    "#   {'str': 'and', 'hit': [84, 3, 20, 1], 'tag': 'CC'},\n",
    "#   {'str': 'Wilma', 'hit': [88, 5, 21, 1], 'tag': 'NNP'},\n",
    "#   {'str': '.', 'hit': [93, 1, 22, 1], 'tag': '.'},\n",
    "#   {'str': 'Yabba Dabba Do', 'hit': [95, 14, 23, 3], 'tag': 'UH'},\n",
    "#   {'str': '!', 'hit': [109, 1, 26, 1], 'tag': '.'}],\n",
    "#   'entity_list': [{'str': 'Fred',\n",
    "#     'hit': [79, 4, 19, 1],\n",
    "#     'type': {'name': 'person.generic', 'i18n': 'person', 'path': '/'},\n",
    "#     'tag': 'person.generic',\n",
    "#     'tag_i18n': 'person'},\n",
    "#    {'str': 'Wilma.',\n",
    "#     'hit': [88, 6, 21, 2],\n",
    "#     'type': {'name': 'person.generic', 'i18n': 'person', 'path': '/'},\n",
    "#     'tag': 'person.generic',\n",
    "#     'tag_i18n': 'person'}],\n",
    "#   'syntactic_parsing_str': '',\n",
    "#   'srl_str': '',\n",
    "#   'echo_data': 0\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:30.506110Z",
     "start_time": "2021-11-23T06:47:09.626227Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern_words = set(pattern_words)\n",
    "for pieces in datasets:\n",
    "    for i, p in enumerate(tqdm(pieces)):\n",
    "        res = handle_a_text(i, p['content'])\n",
    "        assert res['echo_data'] == i\n",
    "        words = []\n",
    "\n",
    "        try:\n",
    "            for i, w in enumerate(res['word_list']):\n",
    "                word = w['str']\n",
    "                if word in pattern_words:\n",
    "                    type = 'PATTERN'\n",
    "                elif w['tag'] in ['NNP', 'NNPS']:\n",
    "                    type = 'ENTITY'\n",
    "                else:\n",
    "                    type = 'OTHERS'\n",
    "\n",
    "                words.append((word, t))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        p['words'] = words\n",
    "\n",
    "        if len(p['words']) == 0:\n",
    "            print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:47:31.803020Z",
     "start_time": "2021-11-23T06:47:30.507729Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(['train', 'val', 'test']):\n",
    "    with open('../../dataset/Twitter/raw/post/{}.json'.format(t), 'w') as f:\n",
    "        json.dump(datasets[i], f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
